{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "\n",
    "import pandas as pd\n",
    "def combine_clinical_data_and_reference_data(reference, clinical):\n",
    "    \"\"\"\n",
    "    This function compare reference data with clinical data, to match variants with a disease identified\n",
    "\n",
    "    Parameter: \n",
    "    reference : the file with the variants of YOUR dataset\n",
    "    clinical : the online database with the clinical data.\n",
    "\n",
    "    Returns: \n",
    "    a dataframe that combine the 2 above parameters.\n",
    "    \"\"\"\n",
    "    # Read the CSV files\n",
    "    df_clinical = pd.read_csv(clinical)\n",
    "    df_REF = reference\n",
    "\n",
    "    # Initialize list for selected indices\n",
    "    selected_indices = []\n",
    "\n",
    "    # Add 'Position' column to df_REF\n",
    "    df_REF['Position'] = df_REF['POS'] \n",
    "\n",
    "    # Loop through each position in df_REF\n",
    "    for pos in df_REF['Position']:\n",
    "        # Find matching indices in df_annotated\n",
    "        matching_indices = df_clinical.index[df_clinical['Position'] == pos].tolist()\n",
    "        if matching_indices:\n",
    "            selected_indices.append(matching_indices)\n",
    "\n",
    "    # Flatten the list of indices\n",
    "    flat_indices = [index[0] for index in selected_indices]\n",
    "\n",
    "    # Filter df_annotated to keep only rows with the selected indices\n",
    "    df_annotated_filtered = df_clinical.loc[flat_indices]\n",
    "\n",
    "    # Correct way to select specific columns\n",
    "    df_annotated_filtered = df_annotated_filtered[['Position', 'Disease']]\n",
    "\n",
    "    df_merged = pd.merge(df_REF, df_annotated_filtered, on='Position', how='left')\n",
    "    df_merged = df_merged.drop(['Position'], axis=1)\n",
    "    return df_merged\n",
    "\n",
    "def clean_headers_of_df(filepath, file_id):\n",
    "    \"\"\"\n",
    "    Clean the headers of a VCF file.\n",
    "    \"\"\"\n",
    "    df_vcf = pd.read_csv(filepath, sep='\\t')\n",
    "    # Split the fileID column by all colons and expand into the eight columns\n",
    "    df_vcf[file_id] = df_vcf[file_id].str.strip()\n",
    "    df_vcf[\"Sample\"] = file_id\n",
    "    df_vcf[['GT', 'AD', 'AF', 'DP', 'F1R2', 'F2R1', 'FAD', 'SB']] = df_vcf[file_id].str.split(':', expand=True, n=7)\n",
    "    df_vcf = df_vcf.drop(['ID','QUAL','GT', 'AD', 'DP', 'F1R2', 'F2R1', 'FAD', 'SB', file_id, 'FORMAT', 'INFO'], axis=1)\n",
    "    # print(df_vcf)\n",
    "    return df_vcf\n",
    "\n",
    "\n",
    "\n",
    "def adding_variant_format(df, position):\n",
    "    # Create the 'Mutation' column by concatenating 'REF', 'POS', and 'ALT'\n",
    "    df['Mutation'] = df['REF'] + df['POS'].astype(str) +  df['ALT']\n",
    "    \n",
    "    # Insert the 'Mutation' column at the specified position\n",
    "    df.insert(position, 'Mutation', df.pop('Mutation'))  # Remove and insert at the new position\n",
    "    \n",
    "    return df\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def split_column(df, column_name, text_col_name, number_col_name):    \n",
    "    # Apply a lambda function to split the values in the specified column\n",
    "    df[[text_col_name, number_col_name]] = df[column_name].apply(\n",
    "        lambda value: re.match(r'([a-zA-Z_]+)\\((\\d+\\.?\\d*)\\)', str(value)).groups() if pd.notnull(value) and re.match(r'([a-zA-Z_]+)\\((\\d+\\.?\\d*)\\)', str(value)) else (value, None)\n",
    "    ).apply(pd.Series)\n",
    "    \n",
    "    # Convert the number column to float type\n",
    "    df[number_col_name] = pd.to_numeric(df[number_col_name], errors='coerce')\n",
    "    \n",
    "    # Remove the original column\n",
    "    df.drop(columns=[column_name], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Data Analysis of GATK data: \n",
    "import glob\n",
    "import os\n",
    "\n",
    "def VCF_table_converter(folder_path, output):\n",
    "    for filepath in glob.glob(os.path.join(folder_path, \"*_markdup_filtered.vcf\")): # Adjust the pattern if needed\n",
    "        # Process each file (filepath is the full path to the file)\n",
    "        with open(filepath, 'r') as file: \n",
    "            lines = file.readlines()\n",
    "        with open(filepath, 'w') as wfile:\n",
    "            for line in lines:\n",
    "                if not line.startswith(\"##\"):\n",
    "                    wfile.write(line)\n",
    "        \n",
    "        filename = os.path.basename(filepath)\n",
    "        file_id = filename.split('_markdup_filtered.vcf')[0]\n",
    "        print(file_id)\n",
    "\n",
    "        df_vcf = clean_headers_of_df(filepath)\n",
    "        df_vcf = adding_variant_format(df_vcf, 1)\n",
    "\n",
    "        # Combining the Clinical data with the reference\n",
    "        df_vcf = combine_clinical_data_and_reference_data(df_vcf,\"MutationsCodingControl_MITOMAP_Foswiki.csv\")\n",
    "        # print(df_vcf)\n",
    "        \n",
    "        file_path_to_VEP = \"Output/VEP_txt_files/lofreq_txt\"\n",
    "        VEP_df = pd.read_csv(f\"{file_path_to_VEP}/{file_id}.txt\", sep='\\t', header=0)\n",
    "        # print(VEP_df)\n",
    "        VEP_df = VEP_df[['IMPACT', 'Consequence', 'SYMBOL', 'Gene', 'Feature', 'BIOTYPE', \n",
    "                         'Existing_variation', 'cDNA_position', 'CDS_position','Protein_position', 'Amino_acids', 'Codons', 'SIFT', 'PolyPhen', 'CLIN_SIG' ]]\n",
    "        VEP_df = split_column(VEP_df, 'SIFT', 'SIFT_', 'SIFT_value')\n",
    "        VEP_df = split_column(VEP_df, 'PolyPhen', 'PolyPhen_', 'PolyPhen_value')\n",
    "        VEP_df = VEP_df.reset_index(drop=True)\n",
    "        df_vcf = df_vcf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        df_vcf = df_vcf.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "        VEP_df = VEP_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "        # print(VEP_df)\n",
    "        # print(df_vcf)\n",
    "        df_vcf = pd.concat([df_vcf, VEP_df], axis=1)\n",
    "        print(df_vcf)\n",
    "        # print out all the tsv files as seperate files in specified folders\n",
    "        df_vcf.to_csv(f'{output}/{file_id}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_headers_of_df_lofreq(filepath):\n",
    "    \"\"\"\n",
    "    Clean the headers of a VCF file.\n",
    "    \"\"\"\n",
    "    df_vcf = pd.read_csv(filepath, sep='\\t')\n",
    "    file_id = 'INFO'\n",
    "    # Split the fileID column by all colons and expand into the eight columns\n",
    "    df_vcf[file_id] = df_vcf[file_id].str.strip()\n",
    "    df_vcf[['DP', 'AF', 'SB', 'DB4']] = df_vcf[file_id].str.split(';', expand=True, n=7)\n",
    "    df_vcf = df_vcf.drop(['ID', 'DP', 'SB', 'DB4', 'INFO'], axis=1)\n",
    "    # print(df_vcf)\n",
    "    return df_vcf\n",
    "\n",
    "\n",
    "## Data Analysis of GATK data: \n",
    "import glob\n",
    "import os\n",
    "\n",
    "def VCF_table_converter_lofreq(folder_path, output):\n",
    "    for filepath in glob.glob(os.path.join(folder_path, \"*_markdup_filtered.vcf\")): # Adjust the pattern if needed\n",
    "        # Process each file (filepath is the full path to the file)\n",
    "        # print(f\"Processing file: {filepath}\")\n",
    "        with open(filepath, 'r') as file: \n",
    "            lines = file.readlines()\n",
    "        with open(filepath, 'w') as wfile:\n",
    "            for line in lines:\n",
    "                if not line.startswith(\"##\"):\n",
    "                    wfile.write(line)\n",
    "        \n",
    "        filename = os.path.basename(filepath)\n",
    "        file_id = filename.split('_markdup_filtered.vcf')[0]\n",
    "        print(file_id)\n",
    "\n",
    "        df_vcf = clean_headers_of_df_lofreq(filepath)\n",
    "        df_vcf = adding_variant_format(df_vcf, 1)\n",
    "\n",
    "        # Combining the Clinical data with the reference\n",
    "        df_vcf = combine_clinical_data_and_reference_data(df_vcf,\"MutationsCodingControl_MITOMAP_Foswiki.csv\")\n",
    "        # print(df_vcf)\n",
    "        \n",
    "        file_path_to_VEP = \"Output/VEP_txt_files/lofreq_txt\"\n",
    "        VEP_df = pd.read_csv(f\"{file_path_to_VEP}/{file_id}.txt\", sep='\\t', header=0)\n",
    "        # print(VEP_df)\n",
    "        VEP_df = VEP_df[['IMPACT', 'Consequence', 'SYMBOL', 'Gene', 'Feature', 'BIOTYPE', \n",
    "                         'Existing_variation', 'cDNA_position', 'CDS_position','Protein_position', 'Amino_acids', 'Codons', 'SIFT', 'PolyPhen', 'CLIN_SIG' ]]\n",
    "        VEP_df = split_column(VEP_df, 'SIFT', 'SIFT_', 'SIFT_value')\n",
    "        VEP_df = split_column(VEP_df, 'PolyPhen', 'PolyPhen_', 'PolyPhen_value')\n",
    "        VEP_df = VEP_df.reset_index(drop=True)\n",
    "        df_vcf = df_vcf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        df_vcf = df_vcf.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "        VEP_df = VEP_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "        df_vcf = pd.concat([df_vcf, VEP_df], axis=1)\n",
    "\n",
    "        # print out all the tsv files as seperate files in specified folders\n",
    "        df_vcf.to_csv(f'{output}/{file_id}.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
